{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hFC265XMru8NyovZG-RP9zceMgI-172X",
      "authorship_tag": "ABX9TyNsad50yqU8EhmxZEkTaQr2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evillag/uncertainty_gan/blob/main/CERN_UE_Output_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Vfsi-ed4caCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuSLTKXzcFiS",
        "outputId": "26cb1da9-6581-4212-e8fe-30faeffe0378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lhcb-rich-gan-uncertainty'...\n",
            "remote: Enumerating objects: 210, done.\u001b[K\n",
            "remote: Total 210 (delta 0), reused 0 (delta 0), pack-reused 210 (from 1)\u001b[K\n",
            "Receiving objects: 100% (210/210), 2.94 MiB | 13.57 MiB/s, done.\n",
            "Resolving deltas: 100% (94/94), done.\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!git clone https://gitlab.com/lambda-hse/lhcb-rich-gan-uncertainty.git\n",
        "!mv lhcb-rich-gan-uncertainty/experiments .\n",
        "!mv lhcb-rich-gan-uncertainty/src .\n",
        "!rm -r lhcb-rich-gan-uncertainty/\n",
        "!rm -r sample_data/\n",
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from experiments.efficiency.uncertainty_model_train import train_model\n",
        "from experiments.efficiency.uncertainty_models import uncertainty_mlp\n",
        "from experiments.efficiency.uncertainty_utils import (\n",
        "    efficiency_bands_with_uncertainty, efficiency_momentum_with_uncertainty)\n",
        "from experiments.efficiency.utils import (\n",
        "    efficiency_bands, efficiency_momentum, ensemble_and_ref_model_inference,\n",
        "    ensemble_and_ref_model_inference_on_bands, tf_to_numpy_dataset,\n",
        "    threshold_selection)\n",
        "from src.cramer_gan_trainer import CramerGANTrainer\n",
        "from src.dataset import CramerGANDataset\n",
        "from src.datasets.utils_rich import (get_merged_typed_dataset,\n",
        "                                     parse_dataset_np, parse_example)\n",
        "from src.models.gans.discriminators.fcn_disc import RICHDiscriminator\n",
        "from src.models.gans.generators.fcn_gen import RichMCDropFunc, VirtualEnsembleModel"
      ],
      "metadata": {
        "id": "j4tFqIwachJ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db97a5c2-d2fc-401c-f09a-9e7b38b0a7db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset download and extraction\n",
        "!unzip -qq drive/MyDrive/cern/data/rich.zip"
      ],
      "metadata": {
        "id": "TklyNidOCDK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model checkpoint download and extraction\n",
        "!unzip -qq drive/MyDrive/cern/data/checkpoints_dropout_0.01.zip"
      ],
      "metadata": {
        "id": "VDWma3_Ok0jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PARTICLES = ['proton'] # [\"pion\", \"kaon\", \"muon\", \"proton\"]\n",
        "DROPOUTS = [0.25, 0.3, 0.35, 0.4]\n",
        "ENSEMBLES = [16, 32, 64, 128, 256]\n",
        "NUM_REPS = 10\n",
        "SUB_SAMPLE_SIZE = .3\n",
        "THRESHOLD = 1.0\n",
        "\n",
        "DATA_DIR = 'rich'\n",
        "CHECKPOINT_BASE = 'checkpoints'\n",
        "CKPT_NUMBER = 'ckpt-21'\n",
        "\n",
        "\n",
        "def get_checkpoint_name(particle):\n",
        "    return f'bernoulli_structured_dropout_line_test_cramer_drop_rate_0.01_{particle}'"
      ],
      "metadata": {
        "id": "E6kdyUS4dB53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _split_by_line(df, slope=1, intercept=0):\n",
        "    top_half = df[df['Brunel_ETA'] > df['Brunel_P'] * slope + intercept]\n",
        "    bottom_half = df[df['Brunel_ETA'] <= df['Brunel_P'] * slope + intercept]\n",
        "\n",
        "    top_half = top_half.reset_index(drop=True)\n",
        "    bottom_half = bottom_half.reset_index(drop=True)\n",
        "\n",
        "    return top_half, bottom_half\n",
        "\n",
        "\n",
        "def split_by_line(df_train, df_test):\n",
        "    return _split_by_line(df_train)[0], _split_by_line(df_test)[1]\n",
        "\n",
        "\n",
        "def load_particle_datasets(particle, data_dir=DATA_DIR):\n",
        "    \"\"\" The returned dictionary has this format:\n",
        "        {\n",
        "          \"<particle_name>\": {\n",
        "            'data_train': data_train,\n",
        "            'data_val': data_val,\n",
        "            'scaler': scaler,\n",
        "            'feats_train': feats_train,\n",
        "            'targets_train': targets_train,\n",
        "            'feats_val': feats_val,\n",
        "            'targets_val': targets_val\n",
        "          }\n",
        "        }\n",
        "    \"\"\"\n",
        "    data_train, data_val, scaler = get_merged_typed_dataset(data_dir, particle, dtype=np.float32, log=True,\n",
        "                                                            sample_fn=split_by_line)\n",
        "    feats_train, targets_train, _ = parse_dataset_np(data_train)\n",
        "    feats_val, targets_val, _ = parse_dataset_np(data_val)\n",
        "\n",
        "    print(f'feats_train shape\\t{feats_train.shape}\\n'\n",
        "          f'targets_train shape\\t{targets_train.shape}\\n'\n",
        "          f'feats_val shape  \\t{feats_val.shape}\\n'\n",
        "          f'targets_val shape\\t{targets_val.shape}\\n')\n",
        "\n",
        "    return {\n",
        "        'data_train': data_train,\n",
        "        'data_val': data_val,\n",
        "        'scaler': scaler,\n",
        "        'feats_train': feats_train,\n",
        "        'targets_train': targets_train,\n",
        "        'feats_val': feats_val,\n",
        "        'targets_val': targets_val\n",
        "    }\n",
        "\n",
        "datasets = {particle: load_particle_datasets(particle) for particle in PARTICLES}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zk-zDebdbfh",
        "outputId": "51ebe68b-6bea-42cd-9f17-2eecab788f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading and concatenating datasets:\n",
            "\trich/proton_+_down_2016_.csv\n",
            "\trich/proton_-_down_2016_.csv\n",
            "\trich/proton_-_up_2016_.csv\n",
            "\trich/proton_+_up_2016_.csv\n",
            "splitting to train/val/test\n",
            "fitting the scaler\n",
            "scaler train sample size: 1000000\n",
            "scaler n_quantiles: 100000, time = 1.9124493598937988\n",
            "scaling train set\n",
            "scaling test set\n",
            "converting dtype to <class 'numpy.float32'>\n",
            "feats_train shape\t(454690, 3)\n",
            "targets_train shape\t(454690, 5)\n",
            "feats_val shape  \t(272463, 3)\n",
            "targets_val shape\t(272463, 5)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MonteCarloDroupoutModel:\n",
        "    def __init__(self, particle, dropout_rate,\n",
        "                 log_dir='log_dir_tmp',\n",
        "                 checkpoint_dir=CHECKPOINT_BASE,\n",
        "                 debug=False):\n",
        "\n",
        "        self.particle = particle\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.log_dir = log_dir\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "\n",
        "        print(f'Generating model for {particle} with a dropout rate of {dropout_rate}')\n",
        "\n",
        "        self._gen_config = {\n",
        "            'drop_rate': dropout_rate,\n",
        "            'dropout_type': 'bernoulli',\n",
        "        }\n",
        "\n",
        "        self._generator = RichMCDropFunc(**self._gen_config)\n",
        "        self._generator.build((None, 3))\n",
        "        self._discriminator = RICHDiscriminator()\n",
        "\n",
        "        if debug:\n",
        "            print(\"\\nGenerator:\\n\")\n",
        "            print(self._generator.summary(line_length=96))\n",
        "            print(\"\\nDiscriminator:\\n\")\n",
        "            print(self._discriminator.summary())\n",
        "            print(f\"\\nCheckpoint path: {self.checkpoint_dir}\\n\")\n",
        "\n",
        "        # Model was trained with tensorflow 2.10.1, use the legacy optimizer\n",
        "        self._generator_optimizer = tf.keras.optimizers.legacy.RMSprop(2e-4)\n",
        "        self._discriminator_optimizer = tf.keras.optimizers.legacy.RMSprop(2e-4)\n",
        "\n",
        "        self._trainer_config = {\n",
        "            'generator': self._generator,\n",
        "            'discriminator': self._discriminator,\n",
        "            'generator_optimizer': self._generator_optimizer,\n",
        "            'discriminator_optimizer': self._discriminator_optimizer,\n",
        "            'checkpoint_dir': self.checkpoint_dir,\n",
        "            'log_dir': log_dir\n",
        "        }\n",
        "\n",
        "        trainer = CramerGANTrainer(**self._trainer_config)\n",
        "        # Restore pretrained model\n",
        "        trainer.restore_last()\n",
        "\n",
        "    def str(self):\n",
        "        return f\"{self.particle}_{self.dropout_rate}\"\n",
        "\n",
        "    def get_generator(self) -> VirtualEnsembleModel:\n",
        "        return self._generator"
      ],
      "metadata": {
        "id": "1rxa9QaXd6qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mc_model = MonteCarloDroupoutModel('proton', .01, debug=True)\n",
        "gen1 = mc_model.get_generator()\n",
        "gen1.single_model_inference_mode()\n",
        "gen1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdyAQi-VeHHD",
        "outputId": "8c8b01e7-dee8-4d06-b74d-4787d2a57d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating model for proton with a dropout rate of 0.01\n",
            "Layer 0\n",
            "Layer 1\n",
            "Layer 2\n",
            "Layer 3\n",
            "Layer 4\n",
            "\n",
            "Generator:\n",
            "\n",
            "Model: \"virtual_ensemble_model\"\n",
            "________________________________________________________________________________________________\n",
            " Layer (type)                              Output Shape                          Param #        \n",
            "================================================================================================\n",
            " Inputs (InputLayer)                       [(None, 3)]                           0              \n",
            "                                                                                                \n",
            " NoiseInjection (NoiseInjection)           (None, 67)                            0              \n",
            "                                                                                                \n",
            " Layer_0/Dense (Dense)                     (None, 128)                           8704           \n",
            "                                                                                                \n",
            " Layer_0/LeakyReLU (LeakyReLU)             (None, 128)                           0              \n",
            "                                                                                                \n",
            " Layer_0/DropoutTrain (DropoutTrain)       (None, 128)                           0              \n",
            "                                                                                                \n",
            " Layer_1/Dense (Dense)                     (None, 128)                           16512          \n",
            "                                                                                                \n",
            " Layer_1/LeakyReLU (LeakyReLU)             (None, 128)                           0              \n",
            "                                                                                                \n",
            " Layer_1/DropoutTrain (DropoutTrain)       (None, 128)                           0              \n",
            "                                                                                                \n",
            " Layer_2/Dense (Dense)                     (None, 128)                           16512          \n",
            "                                                                                                \n",
            " Layer_2/LeakyReLU (LeakyReLU)             (None, 128)                           0              \n",
            "                                                                                                \n",
            " Layer_2/DropoutTrain (DropoutTrain)       (None, 128)                           0              \n",
            "                                                                                                \n",
            " Layer_3/Dense (Dense)                     (None, 128)                           16512          \n",
            "                                                                                                \n",
            " Layer_3/LeakyReLU (LeakyReLU)             (None, 128)                           0              \n",
            "                                                                                                \n",
            " Layer_3/DropoutTrain (DropoutTrain)       (None, 128)                           0              \n",
            "                                                                                                \n",
            " Layer_4/Dense (Dense)                     (None, 128)                           16512          \n",
            "                                                                                                \n",
            " Layer_4/LeakyReLU (LeakyReLU)             (None, 128)                           0              \n",
            "                                                                                                \n",
            " Layer_4/DropoutTrain (DropoutTrain)       (None, 128)                           0              \n",
            "                                                                                                \n",
            " DensePrediction (Dense)                   (None, 5)                             645            \n",
            "                                                                                                \n",
            "================================================================================================\n",
            "Total params: 75397 (294.52 KB)\n",
            "Trainable params: 75397 (294.52 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "________________________________________________________________________________________________\n",
            "None\n",
            "\n",
            "Discriminator:\n",
            "\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 3)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 5)]                  0         []                            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 8)                    0         ['input_1[0][0]',             \n",
            "                                                                     'input_2[0][0]']             \n",
            "                                                                                                  \n",
            " sequential (Sequential)     (None, 256)                  100224    ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 100224 (391.50 KB)\n",
            "Trainable params: 100224 (391.50 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "\n",
            "Checkpoint path: checkpoints\n",
            "\n",
            "0.001\n",
            "Last ckpt:  None\n",
            "Model: \"virtual_ensemble_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Inputs (InputLayer)         [(None, 3)]               0         \n",
            "                                                                 \n",
            " NoiseInjection (NoiseInjec  (None, 67)                0         \n",
            " tion)                                                           \n",
            "                                                                 \n",
            " Layer_0/Dense (Dense)       (None, 128)               8704      \n",
            "                                                                 \n",
            " Layer_0/LeakyReLU (LeakyRe  (None, 128)               0         \n",
            " LU)                                                             \n",
            "                                                                 \n",
            " Layer_0/DropoutTrain (Drop  (None, 128)               0         \n",
            " outTrain)                                                       \n",
            "                                                                 \n",
            " Layer_1/Dense (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " Layer_1/LeakyReLU (LeakyRe  (None, 128)               0         \n",
            " LU)                                                             \n",
            "                                                                 \n",
            " Layer_1/DropoutTrain (Drop  (None, 128)               0         \n",
            " outTrain)                                                       \n",
            "                                                                 \n",
            " Layer_2/Dense (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " Layer_2/LeakyReLU (LeakyRe  (None, 128)               0         \n",
            " LU)                                                             \n",
            "                                                                 \n",
            " Layer_2/DropoutTrain (Drop  (None, 128)               0         \n",
            " outTrain)                                                       \n",
            "                                                                 \n",
            " Layer_3/Dense (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " Layer_3/LeakyReLU (LeakyRe  (None, 128)               0         \n",
            " LU)                                                             \n",
            "                                                                 \n",
            " Layer_3/DropoutTrain (Drop  (None, 128)               0         \n",
            " outTrain)                                                       \n",
            "                                                                 \n",
            " Layer_4/Dense (Dense)       (None, 128)               16512     \n",
            "                                                                 \n",
            " Layer_4/LeakyReLU (LeakyRe  (None, 128)               0         \n",
            " LU)                                                             \n",
            "                                                                 \n",
            " Layer_4/DropoutTrain (Drop  (None, 128)               0         \n",
            " outTrain)                                                       \n",
            "                                                                 \n",
            " DensePrediction (Dense)     (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 75397 (294.52 KB)\n",
            "Trainable params: 75397 (294.52 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_generator(dropout_rate, dropout_type='bernoulli'):\n",
        "    gen_config = {\n",
        "        'drop_rate': dropout_rate,\n",
        "        'dropout_type': dropout_type,\n",
        "    }\n",
        "    generator = RichMCDropFunc(**gen_config)\n",
        "    generator.build((None, 3))\n",
        "    return generator"
      ],
      "metadata": {
        "id": "QkQtLhOsfm2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution to read embeddings of any layer:\n",
        "\n",
        "# 1. Create a new model with the same architecture\n",
        "new_gen = create_generator(.01)\n",
        "new_gen.set_weights(gen1.get_weights())\n",
        "\n",
        "# 2. Create a new model that outputs from the layer of interest\n",
        "input_layer = new_gen.input\n",
        "output_layer = new_gen.layers[14].output  # Index is 14 because layer indexing starts from 0\n",
        "\n",
        "# 3. Create a `new_model` without optimizations\n",
        "new_model = Model(input_layer, [output_layer, new_gen.output])\n",
        "\n",
        "# 4. Use `new_model` to predict on any input vector and get the embeddings\n",
        "input_data = np.random.rand(1, 3)\n",
        "embedding, prediction = new_model.predict(input_data)\n",
        "\n",
        "print('Layer 4 embedding:', embedding)\n",
        "print('Final prediction:', prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f7964Nsfruv",
        "outputId": "ef8e7403-7562-4d01-cc92-71532eee7528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 0\n",
            "Layer 1\n",
            "Layer 2\n",
            "Layer 3\n",
            "Layer 4\n",
            "1/1 [==============================] - 1s 553ms/step\n",
            "Layer 4 embedding: [[-0.06809298 -0.18732685  0.09716857 -0.11373783  0.00190123  0.290569\n",
            "  -0.18462771 -0.0034719   0.11210608  0.11586802 -0.04847524 -0.03848957\n",
            "   0.19875628  0.15195915 -0.28900376 -0.03553712 -0.05702229  0.13993752\n",
            "   0.08076362  0.08133171  0.1166122  -0.3413875  -0.06982535  0.236406\n",
            "  -0.03750167  0.12933367 -0.1591197   0.15464732  0.10413228  0.15769379\n",
            "  -0.10908235 -0.10817184  0.14233832 -0.10946845 -0.31025255  0.04373488\n",
            "  -0.14985712 -0.088806    0.24635313 -0.11679609  0.20422535  0.10386544\n",
            "  -0.05323229  0.1349889  -0.15639372 -0.234209    0.07865154 -0.22183321\n",
            "   0.09174708  0.06153359 -0.03441417  0.1776678  -0.05372012  0.0345464\n",
            "   0.17155069 -0.08633559 -0.40486044 -0.24406838  0.23207858 -0.01874956\n",
            "  -0.133167   -0.22324546  0.1348783  -0.06088768  0.23631108  0.39888772\n",
            "   0.32739127 -0.09040491  0.00548531  0.2437619  -0.26109856 -0.00204775\n",
            "   0.13505948  0.12771133 -0.3024594  -0.20442452  0.12383948 -0.06562365\n",
            "   0.05818917  0.10397466 -0.20719787  0.05856264  0.19887482 -0.19977997\n",
            "  -0.01187668 -0.3566361  -0.37217894  0.25239748  0.27566993  0.19761634\n",
            "   0.6315157  -0.21344396  0.07719514 -0.04329307 -0.18371564 -0.01462886\n",
            "   0.10330036  0.15269336  0.09101887  0.02369014  0.13100855 -0.07388189\n",
            "   0.23583853  0.2745395  -0.10750434 -0.11446099  0.07105877  0.26430976\n",
            "   0.01988687  0.08908965 -0.04491899  0.1457911  -0.05825702  0.07399066\n",
            "   0.26709306  0.2326071  -0.1662407   0.21389535 -0.10128627  0.19274516\n",
            "   0.3285896  -0.50291914 -0.4301724  -0.15178902  0.19017047 -0.3594674\n",
            "  -0.1126285   0.17880896]]\n",
            "Final prediction: [[-0.11857834  0.31150603 -0.08276823 -0.14511453 -0.01240555]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = 'drive/MyDrive/Colab Notebooks/outputs/proton/'\n",
        "\n",
        "dataset = datasets['proton']\n",
        "\n",
        "train_feats = dataset['feats_train']\n",
        "train_targets = dataset['targets_train']\n",
        "test_feats = dataset['feats_val']\n",
        "test_targets = dataset['targets_val']\n",
        "\n",
        "print('Train features shape:', train_feats.shape)\n",
        "print('Train targets shape:', train_targets.shape)\n",
        "print('Test features shape:', test_feats.shape)\n",
        "print('Test targets shape:', test_targets.shape)\n",
        "\n",
        "np.save(dir + 'train_feats.npy', train_feats)\n",
        "np.save(dir + 'train_targets.npy', train_targets)\n",
        "np.save(dir + 'test_feats.npy', test_feats)\n",
        "np.save(dir + 'test_targets.npy', test_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qjt-dAjTieWX",
        "outputId": "461fbdae-ee3f-44a4-972c-e30399edec04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train features shape: (454690, 3)\n",
            "Train targets shape: (454690, 5)\n",
            "Test features shape: (272463, 3)\n",
            "Test targets shape: (272463, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_embeddings, train_predictions = new_model.predict(train_feats)\n",
        "test_embeddings, test_predictions = new_model.predict(test_feats)\n",
        "\n",
        "np.save(dir + 'train_embeddings.npy', train_embeddings)\n",
        "np.save(dir + 'train_predictions.npy', train_predictions)\n",
        "np.save(dir + 'test_embeddings.npy', test_embeddings)\n",
        "np.save(dir + 'test_predictions.npy', test_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqcVEKBEhqn7",
        "outputId": "26044d2d-5cc9-495a-efe2-05b7f3bc0dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14210/14210 [==============================] - 85s 6ms/step\n",
            "8515/8515 [==============================] - 49s 6ms/step\n"
          ]
        }
      ]
    }
  ]
}